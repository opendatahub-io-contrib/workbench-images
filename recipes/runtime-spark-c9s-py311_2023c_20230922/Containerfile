################# 
# Spark builder #
#################

FROM workbench-images:base-c9s-py311_2023c_20230922 as builder

# Build options
ARG SPARK_VERSION=3.4.1
ARG HADOOP_VERSION=3.3.6
ARG JMX_PROMETHEUS_JAVAAGENT_VERSION=0.19.0
# Spark's Guava version to match with Hadoop's
ARG GUAVA_VERSION=32.1.1-jre

USER 0

WORKDIR /

# Install gzip to extract archives
RUN dnf install -y gzip && \
    dnf clean all

# Download Spark
ADD https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-without-hadoop.tgz .
# Unzip Spark
RUN tar -xvzf spark-${SPARK_VERSION}-bin-without-hadoop.tgz --no-same-owner
RUN mv spark-${SPARK_VERSION}-bin-without-hadoop spark

# Download Hadoop
ADD https://archive.apache.org/dist/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz .
# Unzip Hadoop
RUN tar -xvzf hadoop-${HADOOP_VERSION}.tar.gz --no-same-owner
RUN mv hadoop-${HADOOP_VERSION} hadoop
# Delete unnecessary hadoop documentation
RUN rm -rf hadoop/share/doc

# Download JMX Prometheus javaagent jar
ADD https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/${JMX_PROMETHEUS_JAVAAGENT_VERSION}/jmx_prometheus_javaagent-${JMX_PROMETHEUS_JAVAAGENT_VERSION}.jar /prometheus/
RUN chmod 0644 prometheus/jmx_prometheus_javaagent*.jar

# Add updated Guava
WORKDIR /spark/jars
RUN rm -f guava-*.jar
ADD https://repo1.maven.org/maven2/com/google/guava/guava/${GUAVA_VERSION}/guava-${GUAVA_VERSION}.jar .

# Add Spark Hadoop Cloud to interact with cloud infrastructures
ADD https://repo1.maven.org/maven2/org/apache/spark/spark-hadoop-cloud_2.12/${SPARK_VERSION}/spark-hadoop-cloud_2.12-${SPARK_VERSION}.jar .

# Build options
ARG SPARK_VERSION=3.4.1
ARG HADOOP_VERSION=3.3.6
ARG JMX_PROMETHEUS_JAVAAGENT_VERSION=0.19.0
# Spark's Guava version to match with Hadoop's
ARG GUAVA_VERSION=32.1.1-jre

FROM workbench-images:base-c9s-py311_2023c_20230922

LABEL name="workbench-images:runtime-spark-c9s-py311_2023c_20230922" \
    summary="runtime-spark  image with Python py311 based on c9s" \
    description="runtime-spark  image with Python py311 based on c9s" \
    io.k8s.description="runtime-spark  image  with Python py311 based on c9s for ODH or RHODS" \
    io.k8s.display-name="runtime-spark  image  with Python py311 based on c9s" \
    authoritative-source-url="https://github.com/opendatahub-contrib/workbench-images" \
    io.openshift.build.commit.ref="2023c" \
    io.openshift.build.source-location="https://github.com/opendatahub-contrib/workbench-images" \
    io.openshift.build.image="https://quay.io/opendatahub-contrib/workbench-images:runtime-spark-c9s-py311_2023c_20230922"

###########################
# Deploy OS Packages      #
###########################

USER 0

WORKDIR /opt/app-root/bin/

COPY --chown=1001:0 os-packages.txt ./

RUN yum install -y $(cat os-packages.txt) && \
    rm -f ./os-packages.txt && \
    yum -y clean all --enablerepo='*' && \
    rm -rf /var/cache/dnf && \
    find /var/log -type f -name "*.log" -exec rm -f {} \;

###########################

##########################################
# Custom Spark Installation from Builder #
##########################################

ARG JAVA_VERSION=11
ARG PKG_ROOT=/opt/app-root
ARG SPARK_VERSION=3.4.1
ARG HADOOP_VERSION=3.3.6
ARG JMX_PROMETHEUS_JAVAAGENT_VERSION=0.19.0

USER 0

WORKDIR ${PKG_ROOT}/${SPARK_VERSION}

####################
# Spark            #
####################

# Copy Spark from builder stage
COPY --from=builder --chown=default:root /spark ${PKG_ROOT}/spark-${SPARK_VERSION}
COPY --from=builder --chown=default:root /spark/kubernetes/dockerfiles/spark/entrypoint.sh /opt/app-root/bin

# Copy Hadoop from builder stage
COPY --from=builder --chown=default:root /hadoop ${PKG_ROOT}/hadoop-${HADOOP_VERSION}

# Copy Prometheus jars from builder stage
COPY --from=builder --chown=default:root /prometheus ${PKG_ROOT}/prometheus

# Setup required env vars for spark and hadoop
ENV JAVA_HOME=/usr/lib/jvm/jre
ENV SPARK_HOME=${PKG_ROOT}/spark-${SPARK_VERSION}
ENV HADOOP_HOME ${PKG_ROOT}/hadoop-${HADOOP_VERSION}

ENV SPARK_DIST_CLASSPATH="$HADOOP_HOME/etc/hadoop:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar:$HADOOP_HOME/share/hadoop/tools/lib/*"

ENV SPARK_EXTRA_CLASSPATH="$SPARK_DIST_CLASSPATH"

ENV LD_LIBRARY_PATH /lib64

ENV PATH="${PATH}:${PKG_ROOT}/spark-${SPARK_VERSION}/bin"

WORKDIR /opt/app-root/src

##########################
# Deploy Python packages #
##########################

USER 1001

WORKDIR /opt/app-root/bin/

# Copy packages list
COPY --chown=1001:0 requirements.txt ./

# Install packages and cleanup
# (all commands are chained to minimize layer size)
RUN echo "Installing softwares and packages" && \
    # Install Python packages \
    pip install --no-cache-dir -r requirements.txt && \
    # Fix permissions to support pip in Openshift environments \
    chmod -R g+w /opt/app-root/lib/python3.11/site-packages && \
    fix-permissions /opt/app-root -P

WORKDIR /opt/app-root/src/

##########################

###########################
# Deploy Runtime packages #
###########################

USER 1001

WORKDIR /opt/app-root/bin

# Copy notebook launcher and utils
COPY --chown=1001:0 utils utils/

WORKDIR /opt/app-root/src


